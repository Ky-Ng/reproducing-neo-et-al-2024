{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb47f07",
   "metadata": {},
   "source": [
    "# Intro to Forward Hooks\n",
    "- learning how to write hooks in Pytorch from first principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f0e8d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd2f1f",
   "metadata": {},
   "source": [
    "## Logging Hook Example\n",
    "- Hook on `nn.Linear` to print out activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "790589ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=3, out_features=4) # Simple Up projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "95d40945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Forward Hooks: OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# We can see that there are no forward hooks\n",
    "print(\"Current Forward Hooks:\", layer._forward_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c15924",
   "metadata": {},
   "source": [
    "### Forward Hook Format\n",
    "- Hooks have the signature `my_hook(module: nn.Module, inpu: torch.Tensor, output: torch.Tensor) -> None`\n",
    "    - Note: if we return None this means we do not patch the forward activation\n",
    "- register the hook using `nn.Module.register_forward_hook(<Hook_Name>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "63147339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hook(module: nn.Module, input: torch.Tensor, output: torch.Tensor):\n",
    "    print(\"Inside hook!\")\n",
    "    print(\"Module:\", module)\n",
    "    print(\"Input shape:\", input[0].shape)   # input is a tuple\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "be8bd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hook_handler = layer.register_forward_hook(print_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a12d85f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Forward Hooks: OrderedDict({43: <function print_hook at 0x10fba0720>})\n"
     ]
    }
   ],
   "source": [
    "# We can see that one hook is registered\n",
    "print(\"Current Forward Hooks:\", layer._forward_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8f7a3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside hook!\n",
      "Module: Linear(in_features=3, out_features=4, bias=True)\n",
      "Input shape: torch.Size([4, 3])\n",
      "Output shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((4,3))\n",
    "y = layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ba47f",
   "metadata": {},
   "source": [
    "### Cleaning up the hook\n",
    "- Prevent memory leaks by calling `<handler>. remove()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4257e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hook_handler.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "823b5fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Forward Hooks: OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# We should see the registered hooks be empty\n",
    "print(\"Current Forward Hooks:\", layer._forward_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7987d2c",
   "metadata": {},
   "source": [
    "## Patching Hook Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cc8ce653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ablation_hook(colum_to_zero: int):\n",
    "    def ablation_hook(module: nn.Module, input: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n",
    "        mask = torch.ones_like(output)\n",
    "        mask[:, colum_to_zero] = 0\n",
    "        return output * mask\n",
    "    return ablation_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "96ad92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.Linear(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "087d2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_handler = w.register_forward_hook(create_ablation_hook(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "51f6e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({44: <function create_ablation_hook.<locals>.ablation_hook at 0x1064e09a0>})\n"
     ]
    }
   ],
   "source": [
    "print(w._forward_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "dce870c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7583,  0.8632, -0.0000, -0.4746],\n",
      "        [-0.6353,  0.8443, -0.0000, -0.2988],\n",
      "        [-0.6618,  0.5972, -0.0000, -0.4115],\n",
      "        [-0.5793,  0.5992, -0.0000, -0.3042]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = w(torch.rand(4,3))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e8b79900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "ablation_handler.remove()\n",
    "print(w._forward_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc5855",
   "metadata": {},
   "source": [
    "## Saving Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "041d063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "def save_activation(name: str) -> None:\n",
    "    def create_save_neuron_hook(module: nn.Module, input: torch.Tensor, output: torch.Tensor) -> None:\n",
    "        print(f\"saving activations of shape {output.shape} to {name}\")\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return create_save_neuron_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "81bdab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = w.register_forward_hook(save_activation(\"layer_w_activation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d25e383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving activations of shape torch.Size([5, 4]) to layer_w_activation\n"
     ]
    }
   ],
   "source": [
    "y = w(torch.rand(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e81cf522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_w_activation': tensor([[-0.7699,  0.7086, -0.2821, -0.5579],\n",
      "        [-0.7109,  0.6113, -0.6538, -0.6928],\n",
      "        [-0.7080,  0.7207, -0.5546, -0.4953],\n",
      "        [-0.5865,  0.6991, -0.3976, -0.1139],\n",
      "        [-0.6517,  0.6065, -0.5421, -0.4833]])}\n"
     ]
    }
   ],
   "source": [
    "print(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b12e0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
